# Mi-Go

## Description
Mi-Go is an open-source test framework designed to evaluate and compare the accuracy of speech-to-text models. It provides a flexible and extensible approach to generating test plans and executing tests on various datasets, including YouTube videos, podcasts, and other audio sources. With Mi-Go, users can easily benchmark different models, compare their performance, and identify areas for improvement.

Mi-Go is suitable for researchers, developers, and data scientists working in the field of speech recognition. It is designed to simplify the testing process and provide a standardized framework for evaluating and comparing speech-to-text models.

Currently, the framework mainly focuses on YouTube datasets, but it can be easily extended to other datasets.

# Framework structure
## Testplan generator
The Testplan generator is a script that generates test plans as JSON files for a specific TestRunner. 

**YouTube testplan generator** is an example of such a generator designed for testing on YouTube videos. It uses the YouTube API to fetch videos and youtube-transcript-api to obtain their transcripts. Users can filter videos by language, category, duration, and other criteria.

## TestRunner
The [TestRunner](https://github.com/Kowalski1024/speech-to-text-tester/blob/master/testrunners/runner_base.py) is a script that executes tests on a specific dataset/testplan, meaning it is designed to work with a particular dataset and testplan. It prepares the data for testing, runs the tests, and stores the results in a database. The Test runner features a register decorator that allows users to easily add new transcript test to the class so they can be easily chosen in command-line arguments.

**YouTube TestRunner** is an example of such a runner designed for testing on YouTube videos. It uses the testplan generated by the YouTube testplan generator and prepares the data by downloading audio and subtitles. Results are stored in a SQLite database and also in a JSON file, which has the same structure as the testplan but with results so it can be used to generate the next testplan iteration.

## TranscriptTest
The [TranscriptTest](https://github.com/Kowalski1024/speech-to-text-tester/blob/master/testrunners/tests/test_base.py) is a model-specific script that runs the speech-to-text model and compares its output to the ground-truth transcript.

**Transcript difference** is an example of such a transcript test designed for testing Whisper models but can be used for other models as well. It normalizes the transcripts using a built-in Whisper normalization function and uses a basic comparison function that leverages Python difflib to compare the transcripts.

## Libs
The [Libs folder](https://github.com/Kowalski1024/speech-to-text-tester/tree/master/lib) contains various helper functions and classes used by the framework, like basic normalization or comparison functions, database classes, and others.


<!-- ## Usage

### Docker

Build image

```shell
docker build -t model-tester .
```

Run container

```shell
docker run  -e GoogleAPI=<YOUR KEY> --gpus all -d --name whisper-tester -it model-tester
```

### Example -->

